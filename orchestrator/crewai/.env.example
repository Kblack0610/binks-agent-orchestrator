# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:405b

# Cluster Configuration
KUBECONFIG_PATH=~/.kube/config
CLUSTER_CONTEXT=default

# API Server Configuration
API_HOST=0.0.0.0
API_PORT=8000

# Agent Configuration
AGENT_SPAWN_NAMESPACE=ai-agents
CLUSTER_OLLAMA_URL=http://ollama-service.ai-services.svc.cluster.local:11434

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/orchestrator.log

# Optional: External services
# GITHUB_TOKEN=your_github_token
# SLACK_WEBHOOK_URL=your_slack_webhook
