apiVersion: batch/v1
kind: Job
metadata:
  name: code-reviewer-agent
  namespace: ai-agents
  labels:
    agent-type: code-reviewer
    spawned-by: binks-orchestrator
spec:
  backoffLimit: 2
  ttlSecondsAfterFinished: 3600  # Clean up 1 hour after completion
  template:
    metadata:
      labels:
        agent-type: code-reviewer
    spec:
      restartPolicy: OnFailure
      containers:
      - name: code-reviewer
        image: python:3.11-slim
        command: ["/bin/bash"]
        args:
          - -c
          - |
            # Install dependencies
            pip install --no-cache-dir crewai langchain-community requests

            # Run the code reviewer agent
            python /app/code_reviewer_agent.py
        env:
        - name: OLLAMA_API_URL
          value: "http://ollama-service.ai-services.svc.cluster.local:11434"
        - name: TASK_ID
          value: "{{ TASK_ID }}"  # Template variable - replaced by orchestrator
        - name: REPO_URL
          value: "{{ REPO_URL }}"  # Template variable
        volumeMounts:
        - name: agent-script
          mountPath: /app
      volumes:
      - name: agent-script
        configMap:
          name: code-reviewer-agent-script
---
# ConfigMap containing the agent script
# In a real setup, you'd build a Docker image with the script instead
apiVersion: v1
kind: ConfigMap
metadata:
  name: code-reviewer-agent-script
  namespace: ai-agents
data:
  code_reviewer_agent.py: |
    #!/usr/bin/env python3
    """
    Code Reviewer Worker Agent
    This agent is spawned by the M3 Orchestrator to review code changes.
    """
    import os
    from crewai import Agent, Task, Crew
    from langchain_community.llms import Ollama

    # Get configuration from environment
    ollama_url = os.getenv('OLLAMA_API_URL')
    task_id = os.getenv('TASK_ID')
    repo_url = os.getenv('REPO_URL')

    print(f"Code Reviewer Agent starting...")
    print(f"Task ID: {task_id}")
    print(f"Repo URL: {repo_url}")

    # Connect to cluster Ollama service
    ollama_llm = Ollama(
        model="llama3:8b",
        base_url=ollama_url
    )

    # Define the agent
    reviewer = Agent(
        role="Code Reviewer",
        goal=f"Review code changes in {repo_url}",
        backstory="You are an expert code reviewer focused on quality and best practices.",
        llm=ollama_llm,
        verbose=True
    )

    # Define the task
    review_task = Task(
        description=f"Clone {repo_url}, review recent changes, and provide feedback.",
        expected_output="A summary of code review findings.",
        agent=reviewer
    )

    # Execute
    crew = Crew(
        agents=[reviewer],
        tasks=[review_task],
        verbose=2
    )

    result = crew.kickoff()

    print("\n=== Code Review Complete ===")
    print(result)

    # In a real setup, you'd send results back to the orchestrator
    # e.g., via an API call or message queue
