# Binks Agent Benchmark Results - 2026-01-28

## Test Environment
- **MCP Server**: agent MCP with filesystem tools
- **Host**: Ollama at 192.168.1.4:11434
- **Test Files**: `apps/pick-a-number/api/`

## Models Tested
| Model | Size | Type |
|-------|------|------|
| qwen3-coder:30b | 30B | Coding specialist |
| llama3.1:70b | 70B | General purpose |
| deepseek-r1:70b | 70B | Reasoning model |
| qwen3-coder:480b | 480B MoE | Large coding specialist |

## Results Summary

### Tier 1: Simple (Single Tool)

**T1.1 - Read File**
| Model | Result | Tool Calls | Time |
|-------|--------|------------|------|
| qwen3-coder:30b | ✅ Pass | 1 | ~1s |
| llama3.1:70b | ✅ Pass | 1 | ~1s |
| deepseek-r1:70b | ✅ Pass | 1 | ~1s |
| qwen3-coder:480b | ✅ Pass | 1 | ~1s |

**Summary**: All models pass simple single-tool tasks reliably.

---

### Tier 2: Multi-Step (Sequential Tools)

**T2.1 - Read → Edit (Version Bump)**
| Model | Result | Tool Calls | Time |
|-------|--------|------------|------|
| qwen3-coder:30b | ✅ Pass | 2 | ~3s |
| llama3.1:70b | ✅ Pass | 2 | ~3s |
| deepseek-r1:70b | ✅ Pass | 2 | 3s |
| qwen3-coder:480b | ✅ Pass | 2 | 3s |

**Summary**: All models successfully plan and execute 2-step sequential tasks.

---

### Tier 3: Complex (Reasoning + Tools)

**T3.1 - Find TODOs → Categorize → Summarize**
| Model | Result | Tool Calls | Time | Notes |
|-------|--------|------------|------|-------|
| qwen3-coder:30b | ✅ Pass | 2 | 5s | Grouped categories, slight duplication |
| llama3.1:70b | ✅ Pass | 2 | 4s | Granular 6-category breakdown |
| deepseek-r1:70b | ✅ Pass | 2 | 5s | Best formatting with descriptions |
| qwen3-coder:480b | ⚠️ Partial | 2 | 4s | Failed first attempt (search pattern), passed on retry |

**Summary**: All models can perform complex analysis tasks, but prompt sensitivity varies.

---

## Key Findings

1. **All models pass Tier 1-2**: Simple and multi-step tasks work reliably across all tested models.

2. **Tier 3 shows model differences**:
   - deepseek-r1:70b produced the best-formatted output
   - llama3.1:70b used more granular categorization
   - qwen3-coder:480b showed prompt sensitivity (failed with one phrasing, passed with another)

3. **Tool efficiency is consistent**: All models used optimal tool call counts (1 for T1, 2 for T2/T3).

4. **Response times scale with model size**: Larger models (480B MoE) don't show significant latency increase for these tasks.

## Recommendations

- **Minimum viable model**: qwen3-coder:30b handles all tested tasks
- **Best quality output**: deepseek-r1:70b for user-facing summaries
- **Most consistent**: llama3.1:70b showed no failures across all tests
- **Prompt engineering**: qwen3-coder:480b may need more explicit instructions

## Next Steps
- [ ] Test T3.2 (Code structure analysis)
- [ ] Test T3.3 (Debug from symptoms)
- [ ] Expand test coverage with edge cases
- [ ] Test with reduced/ambiguous prompts to find capability thresholds
